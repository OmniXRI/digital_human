{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9873df6-1e29-4a36-8601-6606e5e741cc",
   "metadata": {},
   "source": [
    "# Digital Human 虛擬主播 \n",
    "by Jack OmniXRI, 2024/12/12\n",
    "\n",
    "本範例是基於 **Intel OpenVINO 2024.4** 及 **Notebooks 2024.5** 進行測試，主要整合下列內容：  \n",
    "* 文字轉語音(Text to Speech, TTS) - MeloTTS https://github.com/zhaohb/MeloTTS-OV/tree/speech-enhancement-and-npu  \n",
    "* 自動對嘴影片生成 - Wav2Lip  https://github.com/openvinotoolkit/openvino_notebooks/tree/2024.5/notebooks/wav2lip  \n",
    "\n",
    "本範例使用 Gradio 作為操作界面，預設啟動網址為 http://127.0.0.1:7869/  (http://localhost:7869/ ，操作步驟如下：  \n",
    "1. 在文字欄位輸入一段文字，調整語速（50% ~ 200%，預計100%），按下「生成語音」鍵即可透過 MeloTTS 產生一個聲音檔案，預設為 \"ov_en_int8_ZH.wav\"。  \n",
    "2. 接著按下「載入樣本」鍵即可載入一個預設的影片(data_video_sun_5s.mp4)和剛才生成的聲音檔案。這裡亦可直接上傳影片和聲音檔案或開啟網路攝影機直接錄影、錄音再進行合成。  \n",
    "3. 最後按下「生成影片」鍵即可開始使用 Wav2Lip 進行影片生成，即得一個可播放的影片，點擊影片左下方播放鍵就能檢視生成結果。  \n",
    "\n",
    "建議影片長度要大於聲音內容長度，因為影片長度不足時會自動重頭播放，會有不連續跳動感產生。  \n",
    "\n",
    "本範例執行前請先參考 https://github.com/OmniXRI/digital_human 安裝步驟及注意事項。如想深入了解 Gradio 可參考[【vMaker Edge AI專欄 #24】 如何使用 Gradio 快速搭建人工智慧應用圖形化人機界面](https://omnixri.blogspot.com/2024/12/vmaker-edge-ai-24-gradio.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6748d6e2-c290-446e-b2cd-1b788be13dde",
   "metadata": {},
   "source": [
    "## Prerequisites 預安裝\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ea2e109-cbbe-4e8f-95c6-3a001e3fbd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omnixri\\py310_openvino_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from melo.api import TTS\n",
    "import time\n",
    "import gradio as gr\n",
    "\n",
    "#print(os.path.abspath('.'))\n",
    "sys.path.append('./wav2lip') # 為了讓系統找得到 wav2lip 相關函式，所以手動加入相關路徑\n",
    "\n",
    "from notebook_utils import device_widget\n",
    "from ov_inference import ov_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898029d0-9ad3-4e40-9d2a-0101154f80bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MeloTTS 文字轉語音設定及處理函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1fd9db6-3015-4604-8e40-74996d78a20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omnixri\\py310_openvino_env\\lib\\site-packages\\df\\io.py:9: UserWarning: `torchaudio.backend.common.AudioMetaData` has been moved to `torchaudio.AudioMetaData`. Please update the import path.\n",
      "  from torchaudio.backend.common import AudioMetaData\n",
      "C:\\Users\\omnixri\\py310_openvino_env\\lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init tts_ov_ZH\\bert_int8_ZH.xml\n",
      "ov_path : tts_ov_ZH\\tts_int8_ZH.xml\n",
      " > Text split to sentences.\n",
      "我們討如何在 Intel 平台上轉換和優化 artificial intelligence 模型\n",
      " > ===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                    | 0/1 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\omnixri\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.775 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use speech enhance\n",
      "\u001b[32m2024-12-12 21:46:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on torch 2.1.0+cpu\u001b[0m\n",
      "\u001b[32m2024-12-12 21:46:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on host omnixri\u001b[0m\n",
      "\u001b[32m2024-12-12 21:46:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mLoading model settings of DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2024-12-12 21:46:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mUsing DeepFilterNet3 model at C:\\Users\\omnixri\\AppData\\Local\\DeepFilterNet\\DeepFilterNet\\Cache\\DeepFilterNet3\u001b[0m\n",
      "\u001b[32m2024-12-12 21:46:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mInitializing model `deepfilternet3`\u001b[0m\n",
      "\u001b[32m2024-12-12 21:46:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mFound checkpoint C:\\Users\\omnixri\\AppData\\Local\\DeepFilterNet\\DeepFilterNet\\Cache\\DeepFilterNet3\\checkpoints\\model_120.ckpt.best with epoch 120\u001b[0m\n",
      "\u001b[32m2024-12-12 21:46:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mRunning on device cpu\u001b[0m\n",
      "\u001b[32m2024-12-12 21:46:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mDF\u001b[0m | \u001b[1mModel loaded\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeloTTS 文字轉語音共花費: 2491.72 ms\n"
     ]
    }
   ],
   "source": [
    "speed = 1.0 # 調整語速\n",
    "use_ov = True  # True 使用 OpenVINO, False 使用 PyTorch\n",
    "use_int8 = True # True 啟用 INT8 格式\n",
    "speech_enhance = True # True 啟用語音增強模式\n",
    "\n",
    "tts_device = \"CPU\" # 指定 TTS 推論裝置 , \"CPU\" 或 \"GPU\"（這裡指 Intel GPU）\n",
    "bert_device = \"CPU\" # 指定 Bert 推論裝置, \"CPU\" 或 \"GPU\" 或 \"NPU\"\n",
    "lang =  \"ZH\" # 設定語系, EN 英文, ZH 中文(含混合英文、簡繁中文皆可)\n",
    "\n",
    "# 指定測試文字轉語音字串\n",
    "if lang == \"ZH\":\n",
    "    text = \"我們討如何在 Intel 平台上轉換和優化 artificial intelligence 模型\"\n",
    "elif lang == \"EN\":\n",
    "    text = \"For Intel platforms, we explore the methods for converting and optimizing models.\"\n",
    "\n",
    "# 若指定語音增強模式則新增 process_audio() 函式\n",
    "if speech_enhance:\n",
    "    from df.enhance import enhance, init_df, load_audio, save_audio\n",
    "    import torchaudio\n",
    "\n",
    "    # 將輸入聲音檔案處理後轉存到新檔案中\n",
    "    def process_audio(input_file: str, output_file: str, new_sample_rate: int = 48000):\n",
    "        \"\"\"\n",
    "        Load an audio file, enhance it using a DeepFilterNet, and save the result.\n",
    "\n",
    "        Parameters:\n",
    "        input_file (str): Path to the input audio file.\n",
    "        output_file (str): Path to save the enhanced audio file.\n",
    "        new_sample_rate (int): Desired sample rate for the output audio file (default is 48000 Hz).\n",
    "        \"\"\"\n",
    "\n",
    "        model, df_state, _ = init_df()\n",
    "        audio, sr = torchaudio.load(input_file)\n",
    "        \n",
    "        # Resample the WAV file to meet the requirements of DeepFilterNet\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=new_sample_rate)\n",
    "        resampled_audio = resampler(audio)\n",
    "\n",
    "        enhanced = enhance(model, df_state, resampled_audio)\n",
    "\n",
    "        # Save the enhanced audio\n",
    "        save_audio(output_file, enhanced, df_state.sr())\n",
    "\n",
    "# 初始化 TTS\n",
    "model = TTS(language=lang, tts_device=tts_device, bert_device=bert_device, use_int8=use_int8)\n",
    "\n",
    "# 取得語者資訊\n",
    "speaker_ids = model.hps.data.spk2id\n",
    "speakers = list(speaker_ids.keys())\n",
    "\n",
    "# 若指定使用 OpenVINO, 檢查該語系是否已處理過，若無則進行轉換，結果會存在 \\tts_ov_語系 路徑下 \n",
    "if use_ov:\n",
    "    ov_path = f\"tts_ov_{lang}\"\n",
    "    \n",
    "    if not Path(ov_path).exists():\n",
    "        # 將原始模型轉換成 OpenVINO IR (bin+xml) 格式\n",
    "        model.tts_convert_to_ov(ov_path, language= lang) \n",
    "\n",
    "    # 進行模型初始化\n",
    "    model.ov_model_init(ov_path, language = lang) \n",
    "\n",
    "if not use_ov: # 若未使用 OpenVINO\n",
    "     for speaker in speakers:\n",
    "        output_path = 'en_pth_{}.wav'.format(str(speaker))\n",
    "        start = time.perf_counter()\n",
    "        model.tts_to_file(text, speaker_ids[speaker], output_path, speed=speed*0.75, use_ov = use_ov)\n",
    "        end = time.perf_counter()\n",
    "else: # 若使用 OpenVINO\n",
    "    for speaker in speakers:\n",
    "        output_path = 'ov_en_int8_{}.wav'.format(speaker) if use_int8 else 'en_ov_{}.wav'.format(speaker)\n",
    "        start = time.perf_counter()\n",
    "        model.tts_to_file(text, speaker_ids[speaker], output_path, speed=speed, use_ov=use_ov)\n",
    "        \n",
    "        if speech_enhance:\n",
    "            print(\"Use speech enhance\")\n",
    "            process_audio(output_path,output_path)\n",
    "            \n",
    "        end = time.perf_counter()         \n",
    "\n",
    "dur_time = (end - start) * 1000\n",
    "print(f\"MeloTTS 文字轉語音共花費: {dur_time:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818d8e6e-5194-4020-8ac7-1bad0b57ae81",
   "metadata": {},
   "source": [
    "## 聲音自動對嘴生成設定及處理函式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c30b4df8-61fa-4b92-af49-58ef77e0dc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8701ce1975b14939a38c02bce3a89405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=3, options=('CPU', 'GPU', 'NPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "# 設定下拉式選單，以選擇推論裝置 (CPU, GPU, NPU, AUTO) (這裡的GPU是指Intel內顯）\n",
    "device = device_widget() \n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11a6805b-ae24-497d-afcd-354066d1d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "#print(os.path.abspath('.'))\n",
    "sys.path.append('./wav2lip') # 為了讓系統找得到 wav2lip 相關函式，所以手動加入相關路徑\n",
    "\n",
    "OV_FACE_DETECTION_MODEL_PATH = Path(\"models/face_detection.xml\") # 指定人臉偵測模型路徑\n",
    "OV_WAV2LIP_MODEL_PATH = Path(\"models/wav2lip.xml\") # 指定 wav2lip 模型路徑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f85e29a1-9f47-4ddc-a6fa-e1174627286c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading video frames...\n",
      "Number of frames available for inference: 125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omnixri\\openvino_notebooks\\2024_4\\openvino_notebooks\\notebooks\\digital_human\\Wav2Lip\\audio.py:100: FutureWarning: Pass sr=16000, n_fft=800 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.filters.mel(hp.sample_rate, hp.n_fft, n_mels=hp.num_mels,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 405)\n",
      "Length of mel chunks: 123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                    | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face_detect_ov images[0].shape:  (768, 576, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                    | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█████████▌                                                                  | 1/8 [00:19<02:14, 19.27s/it]\u001b[A\n",
      " 25%|███████████████████                                                         | 2/8 [00:37<01:53, 18.91s/it]\u001b[A\n",
      " 38%|████████████████████████████▌                                               | 3/8 [00:56<01:34, 18.81s/it]\u001b[A\n",
      " 50%|██████████████████████████████████████                                      | 4/8 [01:15<01:15, 18.85s/it]\u001b[A\n",
      " 62%|███████████████████████████████████████████████▌                            | 5/8 [01:34<00:56, 18.77s/it]\u001b[A\n",
      " 75%|█████████████████████████████████████████████████████████                   | 6/8 [01:52<00:37, 18.67s/it]\u001b[A\n",
      " 88%|██████████████████████████████████████████████████████████████████▌         | 7/8 [02:11<00:18, 18.61s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 8/8 [02:24<00:00, 18.03s/it]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 1/1 [02:26<00:00, 146.68s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'results/result_voice.mp4'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ov_inference import ov_inference\n",
    "\n",
    "# 確認存放輸出結果路徑是否存在，若否則建立 \\results 路徑\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "# 使用 OpenVINO 進行推論（至少跑一次）\n",
    "ov_inference(\n",
    "    \"data_video_sun_5s.mp4\", # 指定原始影片檔案\n",
    "    \"data_audio_sun_5s.wav\", # 指定聲音檔案\n",
    "    face_detection_path=OV_FACE_DETECTION_MODEL_PATH, # 指定人臉偵測模型路徑\n",
    "    wav2lip_path=OV_WAV2LIP_MODEL_PATH, # 指定 wav2lip 模型路徑\n",
    "    inference_device=device.value, # 指定推論裝置\n",
    "    outfile=\"results/result_voice.mp4\", # 輸出結果檔案名稱\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fafa917-9f8f-47a1-afec-92449320a6e7",
   "metadata": {},
   "source": [
    "## 使用 Gradio 產生互動界面\n",
    "\n",
    "除在欄位上顯示操作界面外，亦可在 http://127.0.0.1:7860 (http://localhost:7860) 以網頁方式呈現。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf940a58-cbca-4ae5-b6ad-e29c093033ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "\n",
    "# 定義文字轉語音處理函式 tts()\n",
    "# 輸入： content（字串）、 use_ov（布林值）、 speed（數值）\n",
    "# 輸出： \"MeloTTS 文字轉語音共花費: xx.xx ms\"（字串）、 語音檔案名稱（字串）\n",
    "def tts(content, use_ov, speed):\n",
    "    start = time.perf_counter()    \n",
    "    model.tts_to_file(content, speaker_ids[speaker], output_path, speed=speed/100, use_ov=use_ov)\n",
    "    \n",
    "    if speech_enhance:\n",
    "            print(\"Use speech enhance\")\n",
    "            process_audio(output_path,output_path)\n",
    "        \n",
    "    end = time.perf_counter()  \n",
    "    dur_time = (end - start) * 1000\n",
    "    audio = \"ov_en_int8_ZH.wav\"\n",
    "    result = f\"MeloTTS 文字轉語音共花費: {dur_time:.2f} ms\"\n",
    "    return result, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2badf95e-1a6f-4fc2-bd38-20d9fa9290a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_example():\n",
    "    video = \"data_video_sun_5s.mp4\"\n",
    "    audio = \"ov_en_int8_ZH.wav\"\n",
    "    return video, audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "590e6a7b-54ea-4f06-9fd3-3b4e9113f2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "IMPORTANT: You are using gradio version 4.26.0, however version 4.44.1 is available, please upgrade.\n",
      "--------\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 設定客製化 Gradio 人機界面\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            # MeloTTS 相關界面\n",
    "            txb_content = gr.Textbox(label=\"文字\", value = \"請輸入文字內容\")\n",
    "            ckb_use_ov = gr.Checkbox(value=True, label=\"Use_OpenCV\")\n",
    "            sld_speed = gr.Slider(50, 200, value=100, label=\"語速(%)\")\n",
    "            txb_cvt_time = gr.Textbox(label=\"轉換時間\")\n",
    "            file_audio = gr.Audio(label=\"輸出結果\", type=\"filepath\")    \n",
    "            \n",
    "            # 設定「生成語音」鍵對應動作\n",
    "            btn_tts = gr.Button(\"生成語音\")\n",
    "            btn_tts.click(tts, \n",
    "                         inputs=[txb_content, ckb_use_ov, sld_speed], \n",
    "                         outputs=[txb_cvt_time, file_audio])\n",
    "        with gr.Column():    \n",
    "            # Wav2Lip 相關界面\n",
    "            face_video = gr.Video(label=\"人臉影片\")\n",
    "            text_audio = gr.Audio(label=\"聲音檔案\", type=\"filepath\")\n",
    "            \n",
    "            # 設定「載入樣本」鍵對應動作\n",
    "            btn_tts = gr.Button(\"載入樣本\")\n",
    "            btn_tts.click(load_example, \n",
    "                         outputs=[face_video, text_audio])\n",
    "        with gr.Column():\n",
    "            output_video = gr.Video(label=\"結果影片\")\n",
    "        \n",
    "            # 設定「生成影片」鍵對應動作\n",
    "            btn_wav2lip = gr.Button(\"生成影片\")\n",
    "            btn_wav2lip.click(ov_inference, \n",
    "                              inputs=[face_video, text_audio],\n",
    "                              outputs=output_video)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53bdb7a-93e4-4ef8-b207-0b078d5ca018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
